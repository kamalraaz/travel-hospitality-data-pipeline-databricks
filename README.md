# travel-hospitality-data-pipeline-databricks
End-to-end Databricks ETL Pipeline using Medallion Architecture, Delta Lake, PySpark, Metadata-driven Incremental Ingestion, and SCD2 Customer Dimension.


This project simulates a real-world Travel & Hospitality Booking System where daily customer and booking data arrives as CSV files.
The goal is to build a fully automated, fault-tolerant, and production-ready data pipeline using:
Databricks
Delta Lake
PySpark
Unity Catalog
PyDeequ (Data Quality)
SCD Type 2
Metadata-driven incremental ingestion

This pipeline ensures:
âœ” No missed daily files
âœ” Handles late-arriving data
âœ” Maintains historical changes in customer information
âœ” ACID-compliant Delta tables
âœ” Daily automated runs through Databricks Workflows

 Architecture â€” Medallion Design
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚        BRONZE            â”‚
                â”‚ Raw CSV files in DBFS    â”‚
                â”‚ bookings_YYYY-MM-DD.csv  â”‚
                â”‚ customers_YYYY-MM-DD.csv â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚ Ingest
                            â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚          SILVER          â”‚
                â”‚ Clean + DQ + SCD2 Merge  â”‚
                â”‚ customer_scd (Delta)     â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚ Join + Agg
                            â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚          GOLD            â”‚
                â”‚ booking_fact (Delta)     â”‚
                â”‚ Aggregated Fact Table    â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Key Features
âœ… 1. Metadata-Driven Incremental Ingestion

A Delta table (pipeline_metadata) tracks last_processed_date

Pipeline auto-detects unprocessed files in DBFS

No file is ever skipped

Late-arriving files automatically processed

Avoids duplicates & reprocessing

âœ… 2. SCD Type 2 Customer Dimension

Implemented using Delta Lake MERGE:

Tracks historical changes in customer profile

Maintains valid_from, valid_to

Closes old record and inserts new version

Fully ACID compliant

âœ… 3. Gold-Layer Booking Fact Table

Daily booking data is transformed and aggregated:

Total revenue

Total quantity

Per customer + booking type metrics

Stored as a Delta table â€” optimized for reporting and BI dashboards.

âœ… 4. Data Quality (PyDeequ)

Before processing data:

Row count checks

Uniqueness checks

Completeness checks

Non-negative numeric validation

Pipeline fails fast if DQ fails.
Ensures only clean data enters Silver/Gold layers.

âœ… 5. Databricks Workflows (Automation)

Daily scheduled run

Automatic cluster management

Log tracking + alerting

Parameterless execution (metadata controls ingestion)

ğŸ“ Project Structure
travel-hospitality-data-pipeline-databricks/
â”‚
â”œâ”€â”€ README.md
â”œâ”€â”€ architecture-diagram.png
â”‚
â”œâ”€â”€ notebooks/
â”‚     â””â”€â”€ incremental_booking_data_processing.py
â”‚
â”œâ”€â”€ metadata/
â”‚     â””â”€â”€ pipeline_metadata.sql
â”‚
â”œâ”€â”€ sample_data/
â”‚     â”œâ”€â”€ bookings_2024-07-26.csv
â”‚     â””â”€â”€ customers_2024-07-26.csv
â”‚
â””â”€â”€ scripts/
      â””â”€â”€ create_catalog_and_tables.sql

ğŸ”§ How the Pipeline Works (Step-by-Step)
1ï¸âƒ£ Daily CSV files land in DBFS (Bronze)

Example:

dbfs:/DataEngineering/bookings_daily_data/bookings_2024-07-26.csv
dbfs:/DataEngineering/customers_daily_data/customers_2024-07-26.csv

2ï¸âƒ£ Metadata table stores last processed date
CREATE TABLE IF NOT EXISTS gds_de_bootcamp.default.pipeline_metadata (
    table_name STRING,
    last_processed_date DATE
);


Example record:

booking_customer_pipeline | 1900-01-01

3ï¸âƒ£ Pipeline reads list of all files â†’ picks only missing ones

Steps:

List all files

Extract dates from filenames

Compare with metadata

Process only dates > last_processed_date

4ï¸âƒ£ Apply Data Quality (PyDeequ)

If fails â†’ stop run.

5ï¸âƒ£ Apply Transformations & SCD2 Merge (Silver Layer)

Customer records undergo:

Change detection

SCD2 merge

Insert new historical version

6ï¸âƒ£ Generate Aggregated Fact Table (Gold Layer)

Using PySpark aggregations.

7ï¸âƒ£ Update Metadata Table

After each successful run:

UPDATE pipeline_metadata
SET last_processed_date = '<last_date_processed>'

ğŸ› ï¸ Technologies Used
Layer	Technology
Storage	DBFS, Delta Lake
Compute	Databricks, Spark 3.x
Orchestration	Databricks Workflows
ETL	PySpark
DQ Framework	PyDeequ
Architecture	Medallion (Bronze/Silver/Gold)
Dimension Modeling	SCD Type 2
Catalog	Unity Catalog
ğŸš€ How to Run This Project
1ï¸âƒ£ Upload sample CSV files to DBFS
dbfs:/DataEngineering/bookings_daily_data/
dbfs:/DataEngineering/customers_daily_data/

2ï¸âƒ£ Create metadata table (SQL file included)

Run:

metadata/pipeline_metadata.sql

3ï¸âƒ£ Run the main ETL notebook

notebooks/incremental_booking_data_processing.py

4ï¸âƒ£ Schedule Daily Run

Create a Databricks Workflow:

Task: Run the notebook

Cluster: Existing or new

Schedule: Daily

No parameters required (metadata-driven)

ğŸ“Š Final Output Delta Tables
ğŸ¥‡ Gold Layer
gds_de_bootcamp.default.booking_fact

ğŸ¥ˆ Silver Layer
gds_de_bootcamp.default.customer_scd

ğŸ“Œ Metadata Layer
gds_de_bootcamp.default.pipeline_metadata

ğŸŒŸ Show Support

If you liked this project:

â­ Star the repository
ğŸ” Share on LinkedIn
ğŸ¯ Fork and try adding new features


Contact
Feel free to reach out:
Raj Kamal
Data Engineer
rrajkamal1999@gmail.com
